{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boundary Value Problems:  Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The simplest boundary value problem (BVP) we will run into is the one-dimensional version of Poisson's equation\n",
    "$$\n",
    "    u''(x) = f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usually we solve this equation on a finite interval with either Dirichlet or Neumann boundary condtions.  Because there are two derivatives in the equation we need two boundary conditions to solve the PDE (really and ODE in this case) uniquely.  To start let us consider the following basic problem\n",
    "$$\\begin{aligned}\n",
    "    u''(x) = f(x) ~~~ \\Omega = [a, b] \\\\\n",
    "    u(a) = \\alpha ~~~ u(b) = \\beta.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BVPs of this sort are often the result of looking at the steady-state form of a time dependent PDE.  For instance, if we were considering the steady-state solution to the heat equation\n",
    "$$\n",
    "    u_t(x,t) = \\kappa u_{xx}(x,t) + \\Psi(x,t) ~~~~ \\Omega = [0, T] \\times [a, b] \\\\\n",
    "    u(x, 0) = u^0(x) ~~~ u(a, t) = \\alpha(t) ~~~ u(b, t) = \\beta(t)\n",
    "$$\n",
    "we would solve the equation where $u_t = 0$ and arrive at\n",
    "$$\n",
    "    u''(x) = - \\Psi / \\kappa,\n",
    "$$\n",
    "a version of Poisson's equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In higher spatial dimensions the second derivative turns into a Laplacian.  Notation varies for this but all these are equivalent statements:\n",
    "$$\\begin{aligned}\n",
    "    \\nabla^2 u(\\vec{x}) &= f(\\vec{x}) \\\\\n",
    "    \\Delta u(\\vec{x}) &= f(\\vec{x}) \\\\\n",
    "    \\sum^N_{i=1} u_{x_i x_i} &= f(\\vec{x}).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-Dimensional Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a first approach to solving the one-dimensional Poisson's equation let's break up the domain into `m` points, often called a *mesh* or *grid*.  Our goal is to approximate the unknown function $u(x)$ as the mesh points $x_i$.  First we can relate the number of mesh points `m` to the distance between with\n",
    "$$\n",
    "    \\Delta x = \\frac{1}{m + 1}.\n",
    "$$\n",
    "The mesh points $x_i$ can be written as\n",
    "$$\n",
    "    x_i = a + i \\Delta x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can let $\\Delta x$ vary and many of the formulas above have only minor modifications but we will leave that for homework.  Notationally we will also adopt the notation\n",
    "$$\n",
    "    U_i \\approx u(x_i)\n",
    "$$\n",
    "so that $U_i$ are the approximate solution at the grid points and retain the lower-case $u$ to denote the true solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To simplify our discussion let's consider the ODE\n",
    "$$\n",
    "    u''(x) = f(x) ~~~ \\Omega = [0, 1] \\\\\n",
    "    u(0) = \\alpha ~~~ u(1) = \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Applying the 2nd order, centered difference approixmation for the 2d derivative we have the equation\n",
    "$$\n",
    "    D^2 U_i = \\frac{1}{\\Delta x^2} (U_{i+1} - 2 U_i + U_{i-1})\n",
    "$$\n",
    "so that we end up with the approximate algebraic expression at every grid point of\n",
    "$$\n",
    "    \\frac{1}{\\Delta x^2} (U_{i+1} - 2 U_i + U_{i-1}) = f(x_i)  ~~~ i = 1, 2, 3, \\ldots, m.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note at this point that these algebraic equations are coupled as each $U_i$ depends on its neighbors.  This means we can write these as system of coupled equations\n",
    "$$\n",
    "    A U = F.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Write the system of equations\n",
    "$$\n",
    "    \u001f\\frac{1}{\\Delta x^2} (U_{i+1} - 2 U_i + U_{i-1}) = f(x_i)  ~~~ i = 1, 2, 3, \\ldots, m.\n",
    "$$\n",
    "\n",
    "Note the boundary conditions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "    \\frac{1}{\\Delta x^2} \\begin{bmatrix}\n",
    "    -2 &  1 &    &    &    \\\\\n",
    "     1 & -2 &  1 &    &    \\\\\n",
    "       &  1 & -2 &  1 &    \\\\\n",
    "       &    &  1 & -2 &  1 \\\\\n",
    "       &    &    &  1 & -2 \\\\\n",
    "    \\end{bmatrix} \\begin{bmatrix}\n",
    "        U_1 \\\\ U_2 \\\\ U_3 \\\\ U_4 \\\\ U_5\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        f(x_1) - \\frac{\\alpha}{\\Delta x^2} \\\\ f(x_2) \\\\ f(x_3) \\\\ f(x_4) \\\\ f(x_5) - \\frac{\\beta}{\\Delta x^2} \\\\\n",
    "    \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "Want to solve the BVP\n",
    "$$\n",
    "    u_{xx} = e^x, ~~~~ x \\in [0, 1] ~~~~ \\text{with} ~~~~ u(0) = 0.0, \\text{ and } u(1) = 3\n",
    "$$\n",
    "via the construction of a linear system of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\begin{aligned}\n",
    "    u_{xx} &= e^x \\\\\n",
    "    u_x &= A + e^x \\\\\n",
    "    u &= Ax + B + e^x\\\\\n",
    "    u(0) &= B + 1 = 0 \\Rightarrow B = -1 \\\\\n",
    "    u(1) &= A - 1 + e^{1} = 3 \\Rightarrow A = 4 - e\\\\ \n",
    "    ~\\\\\n",
    "    u(x) &= (4 - e) x - 1 + e^x\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "a = 0.0\n",
    "b = 1.0\n",
    "u_a = 0.0\n",
    "u_b = 3.0\n",
    "f = lambda x: numpy.exp(x)\n",
    "u_true = lambda x: (4.0 - numpy.exp(1.0)) * x - 1.0 + numpy.exp(x)\n",
    "\n",
    "# Descretization\n",
    "m = 10\n",
    "x_bc = numpy.linspace(a, b, m + 2)\n",
    "x = x_bc[1:-1]\n",
    "delta_x = (b - a) / (m + 1)\n",
    "\n",
    "# Construct matrix A\n",
    "A = numpy.zeros((m, m))\n",
    "diagonal = numpy.ones(m) / delta_x**2\n",
    "A += numpy.diag(diagonal * -2.0, 0)\n",
    "A += numpy.diag(diagonal[:-1], 1)\n",
    "A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "# Construct RHS\n",
    "b = f(x)\n",
    "b[0] -= u_a / delta_x**2\n",
    "b[-1] -= u_b / delta_x**2\n",
    "\n",
    "# Solve system\n",
    "U = numpy.empty(m + 2)\n",
    "U[0] = u_a\n",
    "U[-1] = u_b\n",
    "U[1:-1] = numpy.linalg.solve(A, b)\n",
    "\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x_bc, U, 'o', label=\"Computed\")\n",
    "axes.plot(x_bc, u_true(x_bc), 'k', label=\"True\")\n",
    "axes.set_title(\"Solution to $u_{xx} = e^x$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"u(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error Analysis\n",
    "\n",
    "A natural question to ask given our approximation $U_i$ is how close this is to the true solution $u(x)$ at the grid points $x_i$.  To address this we will define the error $E$ as\n",
    "$$\n",
    "    E = U - \\hat{U}\n",
    "$$\n",
    "where $U$ is the vector of the approximate solution and $\\hat{U}$ is the vector composed of the $u(x_i)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This leaves $E$ as a vector still so often we ask the question how does the norm of $E$ behave given a particular $\\Delta x$.  For the $\\infty$-norm we would have\n",
    "$$\n",
    "    ||E||_\\infty = \\max_{1 \\leq i \\leq m} |E_i| = \\max_{1 \\leq i \\leq m} |U_i - u(x_i)|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we can show that $||E||_\\infty$ goes to zero as $\\Delta x \\rightarrow 0$ we can then claim that the approximate solution $U_i$ at any of the grid points $E_i \\rightarrow 0$.  If we would like to use other norms we often define slightly modified versions of the norms that also contain the grid width $\\Delta x$ where\n",
    "$$\\begin{aligned}\n",
    "    ||E||_1 &= \\Delta x \\sum^m_{i=1} |E_i| \\\\\n",
    "    ||E||_2 &= \\left( \\Delta x \\sum^m_{i=1} |E_i|^2 \\right )^{1/2}\n",
    "\\end{aligned}$$\n",
    "These are referred to as *grid function norms*.\n",
    "\n",
    "The $E$ defined above is known as the *global error*.  Our goal now turns to using the local truncation error and some idea of stability to imply the global error goes to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Local Truncation Error\n",
    "\n",
    "The *local truncation error* (LTE) can be defined by replacing the approximate solution $U_i$ by the approximate solution $u(x_i)$.  Since the algebraic equations are an approximation to the original BVP, we do not expect that the true solution will exactly satisfy these equations, this resulting difference is the LTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our one-dimensional finite difference approximation from above we have\n",
    "$$\n",
    "    \u001f\\frac{1}{\\Delta x^2} (U_{i+1} - 2 U_i + U_{i-1}) = f(x_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Replacing $U_i$ with $u(x_i)$ in this equation leads to\n",
    "$$\n",
    "    \u001f\\tau_i = \\frac{1}{\\Delta x^2} (u(x_{i+1}) - 2 u(x_i) + u(x_{i-1})) - f(x_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this form the LTE is not as useful but if we assume $u(x)$ is smooth we can replace the $u(x_i)$ with their Taylor series counterparts, similar to what we did for finite differences.  The relevant Taylor series are\n",
    "$$\n",
    "    u(x_{i \\pm 1}) = u(x_i) \\pm u'(x_i) \\Delta x + \\frac{1}{2} u''(x_i) \\Delta x^2 \\pm \\frac{1}{6} u'''(x_i) \\Delta x^3 + \\frac{1}{24} u^{(4)}(x_i) \\Delta x^4 + \\mathcal{O}(\\Delta x^5)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This leads to an expression for $\\tau_i$ of\n",
    "$$\\begin{aligned}\n",
    "    \u001f\\tau_i &= \\frac{1}{\\Delta x^2} \\left [u''(x_i) \\Delta x^2 + \\frac{1}{12} u^{(4)}(x_i) \\Delta x^4 + \\mathcal{O}(\\Delta x^5) \\right ] - f(x_i) \\\\\n",
    "    &= u''(x_i) + \\frac{1}{12} u^{(4)}(x_i) \\Delta x^2 + \\mathcal{O}(\\Delta x^4) - f(x_i) \\\\\n",
    "    &= \\frac{1}{12} u^{(4)}(x_i) \\Delta x^2 + \\mathcal{O}(\\Delta x^4)\n",
    "\\end{aligned}$$\n",
    "where we note that the true solution would satisfy $u''(x) = f(x)$.\n",
    "\n",
    "As long as $ u^{(4)}(x_i) $ remains finite (smooth) we know that $\\tau_i \\rightarrow 0$ as $\\Delta x \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also write the vector of LTEs as\n",
    "$$\n",
    "    \\tau = A \\hat{U} - F\n",
    "$$\n",
    "which implies\n",
    "$$\n",
    "    A\\hat{U} = F + \\tau.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Global Error\n",
    "\n",
    "What we really want to bound is the global error $E$.  To relate the global error and LTE we can substitute $E = U - \\hat{U}$ into our expression for the LTE to find\n",
    "$$\n",
    "    A E = -\\tau.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that the global error is the solution to the system of equations we defined for the approximation except with $\\tau$ as the forcing function rather than $F$!  This also implies that the global error $E$ can be thought of as an approximation to similar BVP as we started with where\n",
    "$$\n",
    "    e''(x) = -\\tau(x) ~~~ \\Omega = [0, 1] \\\\\n",
    "    e(0) = 0 ~~~ e(1) = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can solve this ODE directly by integrating twice since to find to leading order\n",
    "$$\\begin{aligned}\n",
    "    e(x) &\\approx -\\frac{1}{12} \\Delta x^2 u''(x) + \\frac{1}{12} \\Delta x^2 (u''(0) + x (u''(1) - u''(0))) \\\\\n",
    "    &= \\mathcal{O}(\\Delta x^2) \\\\\n",
    "    &\\rightarrow 0 ~~~ \\text{as} ~~~ \\Delta x \\rightarrow 0.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stability\n",
    "\n",
    "We should that the continuous analog to $E$ $e(x)$ does in fact go to zero as $\\Delta x \\rightarrow 0$ but what about $E$?  Instead of showing something based on $e(x)$ let's look back at the original system of equations for the global error\n",
    "$$\n",
    "    A^{\\Delta x} E^{\\Delta x} = - \\tau^{\\Delta x}\n",
    "$$\n",
    "where we now denote a particular realization of the system by the corresponding grid spacing $\\Delta x$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we could invert $A^{\\Delta x}$ we could compute $E^{\\Delta x}$ directly.  Assuming that we can and taking an appropriate norm we find\n",
    "$$\\begin{aligned}\n",
    "    E^{\\Delta x} &= (A^{\\Delta x})^{-1} \\tau^{\\Delta x} \\\\\n",
    "    ||E^{\\Delta x}|| &= ||(A^{\\Delta x})^{-1} \\tau^{\\Delta x}|| \\\\\n",
    "    & \\leq ||(A^{\\Delta x})^{-1} ||~|| \\tau^{\\Delta x}||\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We know that $\\tau^{\\Delta x} \\rightarrow 0$ as $\\Delta x \\rightarrow 0$ already for our example so if we can bound the norm of the matrix $(A^{\\Delta x})^{-1}$ by some constant $C$ for sufficiently small $\\Delta x$ we can then write a bound on the global error of\n",
    "$$\n",
    "    ||E^{\\Delta x}|| \\leq C ||\\tau^{\\Delta x}||\n",
    "$$\n",
    "demonstrating that $E^{\\Delta x} \\rightarrow 0 $ at least as fast as $\\tau^{\\Delta x} \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can generalize this observation to all linear BVP problems by supposing that we have a finite difference approximation to a linear BVP of the form\n",
    "$$\n",
    "    A^{\\Delta x} U^{\\Delta x} = F^{\\Delta x},\n",
    "$$\n",
    "where $\\Delta x$ is the grid spacing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We say the approximation is *stable* if $(A^{\\Delta x})^{-1}$ exists $\\forall \\Delta x < \\Delta x_0$ and there is a constant $C$ such that\n",
    "$$\n",
    "    ||(A^{\\Delta x})^{-1}|| \\leq C ~~~~ \\forall \\Delta x < \\Delta x_0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Consistency\n",
    "\n",
    "A related and important idea for the discretization of and PDE is that it be consistent with the equation we are approximating.  If\n",
    "$$\n",
    "    ||\\tau^{\\Delta x}|| \\rightarrow 0 ~~\\text{as}~~ \\Delta x \\rightarrow 0\n",
    "$$\n",
    "then we say an approximation is *consistent* with the differential equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convergence\n",
    "\n",
    "We now have all the pieces to say something about the global error $E$.  A method is said to be *convergent* if\n",
    "$$\n",
    "    ||E^{\\Delta x}|| \\rightarrow 0 ~~~ \\text{as} ~~~ \\Delta x \\rightarrow 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If an approximation is both consistent ($||\\tau^{\\Delta x}|| \\rightarrow 0 ~~\\text{as}~~ \\Delta x \\rightarrow 0$) and stable ($||E^{\\Delta x}|| \\leq C ||\\tau^{\\Delta x}||$) then convergence is implied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have only derived this in the case of linear BVPs but in fact these criteria for convergence are often found to be true for any finite difference approximation (and beyond for that matter).  This statement of convergence can also often be strengthened to say\n",
    "$$\n",
    "    \\mathcal{O}(\\Delta x^p) ~\\text{LTE}~ + ~\\text{stability} ~ \\Rightarrow \\mathcal{O}(\\Delta x^p) ~\\text{global error}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out the most difficult part of this process is usually the statement regarding stability.  In the next section we will see for our simple example how we can prove stability in the 2-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stability in the 2-Norm\n",
    "\n",
    "Recalling our definition of stability, we need to show that for our previously defined $A$ that\n",
    "$$\n",
    "    (A^{\\Delta x})^{-1}\n",
    "$$\n",
    "exists and\n",
    "$$\n",
    "    ||(A^{\\Delta x})^{-1}|| \\leq C ~~~ \\forall \\Delta x < \\Delta x_0\n",
    "$$\n",
    "for some $C$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can show that $A$ is in fact invertible but can we bound the norm of the inverse?  Recall that the 2-norm of a symmetric matrix is equal to its spectral radius, i.e.\n",
    "$$\n",
    "    ||A||_2 = \\rho(A) = \\max_{1\\leq p \\leq m} |\\lambda_p|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the inverse of $A$ is also symmetric the eigenvalues of $A^{-1}$ are the inverses of the eigenvalues of $A$ implying that\n",
    "$$\n",
    "    ||A^{-1}||_2 = \\rho(A^{-1}) = \\max_{1\\leq p \\leq m} \\left| \\frac{1}{\\lambda_p} \\right| = \\frac{1}{\\max_{1\\leq p \\leq m} \\left| \\lambda_p \\right|}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If none of the $\\lambda_p$ of $A$ are zero for sufficiently small $\\Delta x$ and that the rest are finite as $\\Delta x \\rightarrow 0$ we have shown the stability of the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The eigenvalues of the matrix $A$ from above can be written as\n",
    "$$\n",
    "    \\lambda_p = \\frac{2}{\\Delta x^2} (\\cos(p \\pi \\Delta x) - 1)\n",
    "$$\n",
    "with the corresponding eigenvectors $v^p$ \n",
    "$$\n",
    "    v^p_j = \\sin(p \\pi j \\Delta x)\n",
    "$$\n",
    "as the $j$th component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check that these are in fact the eigenpairs of the matrix $A$\n",
    "$$\n",
    "    \\lambda_p = \\frac{2}{\\Delta x^2} (\\cos(p \\pi \\Delta x) - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    v^p_j = \\sin(p \\pi j \\Delta x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\begin{aligned}\n",
    "    (A u^p_j) &= \\frac{1}{\\Delta x^2} (v^p_{j-1} - 2 v^p_j + v^p_{j+1} ) \\\\\n",
    "    &= \\frac{1}{\\Delta x^2} (\\sin(p \\pi (j-1) \\Delta x) - 2 \\sin(p \\pi j \\Delta x) + \\sin(p \\pi (j+1) \\Delta x) ) \\\\\n",
    "    &= \\frac{1}{\\Delta x^2} (\\sin(p \\pi j \\Delta x) \\cos(p \\pi \\Delta x) - 2 \\sin(p \\pi j \\Delta x) + \\sin(p \\pi j \\Delta x) \\cos(p \\pi \\Delta x) \\\\\n",
    "    &= \\lambda_p v^p_j.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Compute the smallest eigenvalue\n",
    "$$\n",
    "    \\lambda_p = \\frac{2}{\\Delta x^2} (\\cos(p \\pi \\Delta x) - 1)\n",
    "$$\n",
    "Use a Taylor series to get an idea of how this behaves with respect to $\\Delta x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From these expressions we know that smallest eigenvalue is\n",
    "$$\\begin{aligned}\n",
    "    \\lambda_1 &= \\frac{2}{\\Delta x^2} (\\cos(\\pi \\Delta x) - 1) \\\\\n",
    "    &= \\frac{2}{\\Delta x^2} \\left (-\\frac{1}{2} \\pi^2 \\Delta x^2 + \\frac{1}{24} \\pi^4 \\Delta x^4 + \\mathcal{O}(\\Delta^6) \\right ) \\\\\n",
    "    &= -\\pi^2 + \\mathcal{O}(\\Delta x^2).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Note that this also gives us an error bound as this eigenvalue will also lead to the largest eigenvalue of the inverse matrix.  We can therefore say\n",
    "$$\n",
    "    ||E^{\\Delta x}||_2 \\leq ||(A^{\\Delta x})^{-1}||_2 ||\\tau^{\\Delta x}||_2 \\approx \\frac{1}{\\pi^2} ||\\tau^{\\Delta x}||_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stability in the $\\infty$-Norm\n",
    "\n",
    "The straight forward approach to show that $||E||_\\infty \\rightarrow 0$ as $\\Delta x \\rightarrow 0$ would be to use the matrix bound\n",
    "$$\n",
    "    ||E||_\\infty \\leq \\frac{1}{\\sqrt{\\Delta x}} ||E||_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our example problem we showed that $||E||_2 = \\mathcal{O}(\\Delta x^2)$ so this implies that we at least know that $||E||_\\infty = \\mathcal{O}(\\Delta x^{3/2})$.  This is unfortunate as we expect $||E||_\\infty = \\mathcal{O}(\\Delta x^{2})$ due to the discretization.  In order to alleviate this problem let's go back and consider our definition of stability but this time consider the $\\infty$-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out that our matrix $A$ can be seen as a number of discrete approximations to *Green's functions* in each column.  This is more broadly applicable later on so we will spend some time reviewing the theory of Green's functions and apply them to our simple example problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Green's Functions\n",
    "\n",
    "Consider the BVP with Dirichlet boundary conditions\n",
    "$$\n",
    "    u''(x) = f(x) ~~~~ \\Omega = [0, 1] \\\\\n",
    "    u(0) = \\alpha ~~~~ u(1) = \\beta.\n",
    "$$\n",
    "Pick a fixed point $\\bar{x} \\in \\Omega$, the Green's function $G(x ; \\bar{x})$ solves the BVP above with\n",
    "$$\n",
    "    f(x) = \\delta(x - \\bar{x})\n",
    "$$\n",
    "and $\\alpha = \\beta = 0$.  You could think of this as the result of a steady-state problem of the heat equation with a point-loss of heat somewhere in the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To find the Green's function for our particular problem we can integrate just around the point $\\bar{x}$ near the $\\delta$ function source to find\n",
    "$$\\begin{aligned}\n",
    "    \\int^{\\bar{x} + \\epsilon}_{\\bar{x} - \\epsilon} u''(x) dx &= \\int^{\\bar{x} + \\epsilon}_{\\bar{x} - \\epsilon} \\delta(x - \\bar{x}) dx \\\\\n",
    "    u'(\\bar{x} + \\epsilon) - u'(\\bar{x} - \\epsilon) &= 1\n",
    "\\end{aligned}$$\n",
    "recalling that by definition the integral of the $\\delta$ function must be 1 if the interval of integration includes $\\bar{x}$.  We see that the jump in the derivative at $\\bar{x}$ from the left and right should be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After a bit of algebra we can solve for the Green's function for our model BVP as\n",
    "$$\n",
    "    G(x; \\bar{x}) = \\left \\{ \\begin{aligned}\n",
    "        (\\bar{x} - 1) x & & 0 \\leq x \\leq \\bar{x} \\\\\n",
    "        \\bar{x} (x - 1) & & \\bar{x} \\leq x \\leq 1\n",
    "    \\end{aligned} \\right . .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One important property of linear PDEs (or ODEs) in general is that they exhibit the principle of superposition.  The reason we care about this with Green's functions is that if we have a $f(x)$ composed of two $\\delta$ functions, it turns out the solution is the sum of the corresponding two Green's functions.  For instance if\n",
    "$$\n",
    "    f(x) = \\delta(x - 0.25) + 2 \\delta(x - 0.5)\n",
    "$$\n",
    "then\n",
    "$$\n",
    "    u(x) = G(x ; 0.25) + 2 G(x ; 0.5).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This of course can be extended to an infinite number of $\\delta$  functions so that\n",
    "$$\n",
    "    f(x) = \\int^1_0 f(\\bar{x}) \\delta(x - \\bar{x}) dx\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "    u(x) = \\int^1_0 f(\\bar{x}) G(x ; \\bar{x}) d\\bar{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To incorporate the effects of boundary conditions we can continue to add Green's functions to the solution to find the general solution of our original BVP as\n",
    "$$\n",
    "    u(x) = \\alpha (1 - x) + \\beta x + \\int^1_0 f(\\bar{x}) G(x ; \\bar{x}) d\\bar{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So why did we do all this?  Well the Green's function solution representation above can be thought of as a linear operator on the function $f(x)$.  Written in perhaps more familiar terms we have\n",
    "$$\n",
    "    \\mathcal{A} u = f ~~~~ u = \\mathcal{A}^{-1} f.\n",
    "$$\n",
    "We see now that our linear operator $\\mathcal{A}$ may be equivalent to our discrete matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To proceed we will modify our original matrix $A$ into a slightly different version based on the same discretization.  Instead of moving the boundary terms to the left-hand-side of the equation instead we will introduce two new \"unknowns\", called *ghost cells*, that will be placed at the edges of the grid.  Will will label these $U_0$ and $U_{m+1}$.  In reality we know the value of these points, they are the boundary conditions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The modified system then looks like\n",
    "$$\n",
    "    A = \\frac{1}{\\Delta x^2} \\begin{bmatrix}\n",
    "        \\Delta x^2 & 0  \\\\\n",
    "        1 & -2 & 1 \\\\\n",
    "          &  1 & -2 & 1 \\\\\n",
    "          &    & \\ddots & \\ddots & \\ddots \\\\\n",
    "          &    &        &      1 &     -2 & 1 \\\\\n",
    "          &    &        &        &      1 & -2 & 1 \\\\\n",
    "          &    &        &        &        &  0 & \\Delta x^2\n",
    "    \\end{bmatrix} ~~~ U = \\begin{bmatrix}\n",
    "        U_0 \\\\ U_1 \\\\ \\vdots \\\\ U_m \\\\ U_{m+1}\n",
    "    \\end{bmatrix}~~~~~ F = \\begin{bmatrix}\n",
    "        \\alpha \\\\ f(x_1) \\\\ \\vdots \\\\ f(x_{m}) \\\\ \\beta\n",
    "    \\end{bmatrix}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This has the advantage later that we can implement more general boundary conditions and it isolates the algebraic dependence on the boundary conditions.  The drawbacks are that the matrix no longer has as simple of a form as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's finally turn to the form of the matrix $A^{-1}$.  Introducing a bit more notation, let $A_{j}$ denote the $j$th column and $A_{ij}$ denote the $i$th $j$th element of the matrix $A$.\n",
    "\n",
    "We know that \n",
    "$$\n",
    "    A A^{-1}_j = e_j\n",
    "$$\n",
    "where $e_j$ is the unit vector with $1$ in the $j$th row ($j$th column of the identity matrix).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the above system has some similarities to a discretized version of the Green's function problem.  Here $e_j$ represents the $\\delta$ function, $A$ the original operator, and $A^{-1}_j$ the effect that the $j$th $\\delta$ function (corresponding to the $\\bar{x}$) has on the full solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out that we can write down the inverse matrix directly using Green's functions (see LeVeque for the details) but we end up with\n",
    "$$\n",
    "    A^{-1}_{ij} = \\Delta xG(x_i ; x_j) = \\left \\{ \\begin{aligned}\n",
    "        \\Delta x (x_j - 1) x_i, & & i &= 1,2, \\ldots j \\\\\n",
    "        \\Delta x (x_i - 1) x_j, & & i &= j, j+1, \\ldots , m\n",
    "    \\end{aligned} \\right . .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also write the effective right-hand side of our system as\n",
    "$$\n",
    "    F = \\alpha e_0 + \\beta e_{m+1} + \\sum^m_{j=1} f_j e_j\n",
    "$$\n",
    "and finally the solution as\n",
    "$$\n",
    "    U = \\alpha A^{-1}_{0} + \\beta A^{-1}_{m+1} + \\sum^m_{j=1} f_j A^{-1}_{j}\n",
    "$$\n",
    "whose elements are\n",
    "$$\n",
    "    U_i = \\alpha(1 - x_i) + \\beta x_i + \\Delta x \\sum^m_{j=1} f_j G(x_i ; x_j).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alright, where has all this gotten us?  Well, since we now know what the form of $A^{-1}$ is we may be able to get at the $\\infty$-norm of this matrix.  Recall that the $\\infty$-norm of a matrix (induced from the $\\infty$-norm for a vector is\n",
    "$$\n",
    "    || C ||_\\infty = \\max_{0\\leq i \\leq m+1} \\sum^{m+1}_{j=0} |C_{ij}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that due to the form of the matrix $A^{-1}$ the first row's sum is \n",
    "$$\n",
    "    \\sum^{m+1}_{j=0} A_{0j}^{-1} = 1\n",
    "$$\n",
    "as is the last rows $A^{-1}_{m+1}$.  We also know that for the other rows $A^{-1}_{i,0} < 1$ and $A^{-1}_{i,m+1} < 1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The intermediate rows are also all bounded as\n",
    "$$\n",
    "    \\sum^{m+1}_{j=0} |A^{-1}_{ij}| \\leq 1 + 1 + m \\Delta x < 3\n",
    "$$\n",
    "using the fact we know that\n",
    "$$\n",
    "    \\Delta x = \\frac{1}{m+1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This of course completes our stability wanderings as we can now say definitively that\n",
    "$$\n",
    "    ||A^{-1}||_\\infty < 3 ~~~ \\forall \\Delta x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neumann Boundary Conditions\n",
    "\n",
    "As mentioned before, we can incorporate other types of boundary conditions into our discretization using the modified version of our matrix.  Let's try to do this for our original problem but with one side having Neumann boundary conditions:\n",
    "$$\n",
    "    u''(x) = f(x) ~~~ \\Omega = [-1, 1] \\\\\n",
    "    u(-1) = \\alpha ~~~ u'(1) = \\sigma.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Group Work**\n",
    "\n",
    "$$\n",
    "    u''(x) = f(x) ~~~ \\Omega = [-1, 1] \\\\\n",
    "    u(-1) = \\alpha ~~~ u'(1) = \\sigma.\n",
    "$$\n",
    "\n",
    "$u(x) = -(5 + e) x - (2 + e + e^{-1}) + e^x$\n",
    "\n",
    "Explore implementing the Neumann boundary condition by\n",
    "1. using a one-sided 1st order expression,\n",
    "1. using a centered 2nd order expression, and\n",
    "1. using a one-sided 2nd order expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def solve_mixed_1st_order_one_sided(m):\n",
    "    # Problem setup\n",
    "    a = -1.0\n",
    "    b = 1.0\n",
    "    alpha = 3.0\n",
    "    sigma = -5.0\n",
    "    f = lambda x: numpy.exp(x)\n",
    "\n",
    "    # Descretization\n",
    "    x_bc = numpy.linspace(a, b, m + 2)\n",
    "    x = x_bc[1:-1]\n",
    "    delta_x = (b - a) / (m + 1)\n",
    "\n",
    "    # Construct matrix A\n",
    "    A = numpy.zeros((m + 2, m + 2))\n",
    "    diagonal = numpy.ones(m + 2) / delta_x**2\n",
    "    A += numpy.diag(diagonal * -2.0, 0)\n",
    "    A += numpy.diag(diagonal[:-1], 1)\n",
    "    A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "    # Construct RHS\n",
    "    b = f(x_bc)\n",
    "\n",
    "    # Boundary conditions\n",
    "    A[0, 0] = 1.0\n",
    "    A[0, 1] = 0.0\n",
    "    A[-1, -1] = 1.0 / (delta_x)\n",
    "    A[-1, -2] = -1.0 / (delta_x)\n",
    "\n",
    "    b[0] = alpha\n",
    "    b[-1] = sigma\n",
    "\n",
    "    # Solve system\n",
    "    U = numpy.linalg.solve(A, b)\n",
    "\n",
    "    return U\n",
    "\n",
    "\n",
    "u_true = lambda x: -(5.0 + numpy.exp(1.0)) * x - (2.0 + numpy.exp(1.0) + numpy.exp(-1.0)) + numpy.exp(x)\n",
    "\n",
    "U = solve_mixed_1st_order_one_sided(10)\n",
    "    \n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x_bc, U, 'o', label=\"Computed\")\n",
    "axes.plot(x_bc, u_true(x_bc), 'k', label=\"True\")\n",
    "axes.set_title(\"Solution to $u_{xx} = e^x$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"u(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def solve_mixed_2nd_order_centered(m):\n",
    "    # Problem setup\n",
    "    a = -1.0\n",
    "    b = 1.0\n",
    "    alpha = 3.0\n",
    "    sigma = -5.0\n",
    "    f = lambda x: numpy.exp(x)\n",
    "\n",
    "    # Descretization\n",
    "    x_bc = numpy.linspace(a, b, m + 2)\n",
    "    x = x_bc[1:-1]\n",
    "    delta_x = (b - a) / (m + 1)\n",
    "\n",
    "    # Construct matrix A\n",
    "    A = numpy.zeros((m + 2, m + 2))\n",
    "    diagonal = numpy.ones(m + 2) / delta_x**2\n",
    "    A += numpy.diag(diagonal * -2.0, 0)\n",
    "    A += numpy.diag(diagonal[:-1], 1)\n",
    "    A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "    # Construct RHS\n",
    "    b = f(x_bc)\n",
    "\n",
    "    # Boundary conditions\n",
    "    A[0, 0] = 1.0\n",
    "    A[0, 1] = 0.0\n",
    "    A[-1, -1] = -1.0 / (delta_x)\n",
    "    A[-1, -2] =  1.0 / (delta_x)\n",
    "\n",
    "    b[0] = alpha\n",
    "    b[-1] = delta_x / 2.0 * f(x_bc[-1]) - sigma\n",
    "\n",
    "    # Solve system\n",
    "    U = numpy.linalg.solve(A, b)\n",
    "\n",
    "    return U\n",
    "\n",
    "U = solve_mixed_2nd_order_one_sided(10)\n",
    "    \n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x_bc, U, 'o', label=\"Computed\")\n",
    "axes.plot(x_bc, u_true(x_bc), 'k', label=\"True\")\n",
    "axes.set_title(\"Solution to $u_{xx} = e^x$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"u(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def solve_mixed_2nd_order_one_sided(m):\n",
    "    # Problem setup\n",
    "    a = -1.0\n",
    "    b = 1.0\n",
    "    alpha = 3.0\n",
    "    sigma = -5.0\n",
    "    f = lambda x: numpy.exp(x)\n",
    "    \n",
    "    # Descretization\n",
    "    x_bc = numpy.linspace(a, b, m + 2)\n",
    "    x = x_bc[1:-1]\n",
    "    delta_x = (b - a) / (m + 1)\n",
    "    \n",
    "    # Construct matrix A\n",
    "    A = numpy.zeros((m + 2, m + 2))\n",
    "    diagonal = numpy.ones(m + 2) / delta_x**2\n",
    "    A += numpy.diag(diagonal * -2.0, 0)\n",
    "    A += numpy.diag(diagonal[:-1], 1)\n",
    "    A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "    # Construct RHS\n",
    "    b = f(x_bc)\n",
    "\n",
    "    # Boundary conditions\n",
    "    A[0, 0] = 1.0\n",
    "    A[0, 1] = 0.0\n",
    "    A[-1, -1] = 3.0 / (2.0 * delta_x)\n",
    "    A[-1, -2] = -4.0 / (2.0 * delta_x)\n",
    "    A[-1, -3] = 1.0 / (2.0 * delta_x)\n",
    "\n",
    "    b[0] = alpha\n",
    "    b[-1] = sigma\n",
    "\n",
    "    # Solve system\n",
    "    U = numpy.linalg.solve(A, b)\n",
    "\n",
    "    return U\n",
    "\n",
    "U = solve_mixed_2nd_order_centered(10)\n",
    "    \n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(x_bc, U, 'o', label=\"Computed\")\n",
    "axes.plot(x_bc, u_true(x_bc), 'k', label=\"True\")\n",
    "axes.set_title(\"Solution to $u_{xx} = e^x$\")\n",
    "axes.set_xlabel(\"x\")\n",
    "axes.set_ylabel(\"u(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "a = -1.0\n",
    "b = 1.0\n",
    "alpha = 3.0\n",
    "sigma = -5.0\n",
    "f = lambda x: numpy.exp(x)\n",
    "u_true = lambda x: -(5.0 + numpy.exp(1.0)) * x - (2.0 + numpy.exp(1.0) + numpy.exp(-1.0)) + numpy.exp(x)\n",
    "\n",
    "# Compute the error as a function of delta_x\n",
    "m_range = numpy.arange(10, 200, 20)\n",
    "delta_x = numpy.empty(m_range.shape)\n",
    "error = numpy.empty((m_range.shape[0], 3))\n",
    "for (i, m) in enumerate(m_range):\n",
    "    \n",
    "    x = numpy.linspace(a, b, m + 2)\n",
    "    delta_x[i] = (b - a) / (m + 1)\n",
    "\n",
    "    # Compute solution\n",
    "    U = solve_mixed_1st_order_one_sided(m)\n",
    "    error[i, 0] = numpy.linalg.norm(U - u_true(x), ord=numpy.infty)\n",
    "    U = solve_mixed_2nd_order_one_sided(m)\n",
    "    error[i, 1] = numpy.linalg.norm(U - u_true(x), ord=numpy.infty)\n",
    "    U = solve_mixed_2nd_order_centered(m)\n",
    "    error[i, 2] = numpy.linalg.norm(U - u_true(x), ord=numpy.infty)\n",
    "    \n",
    "titles = [\"1st Order, One-Sided\", \"2nd Order, Centered\", \"2nd Order, One-Sided\"]\n",
    "order_C = lambda delta_x, error, order: numpy.exp(numpy.log(error) - order * numpy.log(delta_x))\n",
    "for i in xrange(3):\n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    axes.loglog(delta_x, error[:, i], 'ko', label=\"Approx. Derivative\")\n",
    "\n",
    "    axes.loglog(delta_x, order_C(delta_x[0], error[0,i], 1.0) * delta_x**1.0, 'r--', label=\"1st Order\")\n",
    "    axes.loglog(delta_x, order_C(delta_x[0], error[0,i], 2.0) * delta_x**2.0, 'b--', label=\"2nd Order\")\n",
    "    axes.legend(loc=4)\n",
    "    axes.set_title(titles[i])\n",
    "    axes.set_xlabel(\"$\\Delta x$\")\n",
    "    axes.set_ylabel(\"$|u(x) - U|$\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "U = solve_mixed_1st_order_one_sided(10)\n",
    "U = solve_mixed_2nd_order_one_sided(10)\n",
    "U = solve_mixed_2nd_order_centered(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Existence and Uniqueness\n",
    "\n",
    "One question that should be asked before embarking upon a numerical solution to any equation is whether the original is *well-posed*.  Well-posedness is defined as a problem that has a unique solution and depends continuously on the input data (inital condition and boundary conditions are examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the BVP we have been exploring but now add strictly Neumann boundary conditions\n",
    "$$\n",
    "    u''(x) = f(x) ~~~ \\Omega = [0, 1] \\\\\n",
    "    u'(0) = \\sigma_0 ~~~ u'(1) = \\sigma_1.\n",
    "$$\n",
    "We can easily discretize this using one of our methods developed above but we run into problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "a = -1.0\n",
    "b = 1.0\n",
    "alpha = 3.0\n",
    "sigma = -5.0\n",
    "f = lambda x: numpy.exp(x)\n",
    "\n",
    "# Descretization\n",
    "m = 50\n",
    "x_bc = numpy.linspace(a, b, m + 2)\n",
    "x = x_bc[1:-1]\n",
    "delta_x = (b - a) / (m + 1)\n",
    "\n",
    "# Construct matrix A\n",
    "A = numpy.zeros((m + 2, m + 2))\n",
    "diagonal = numpy.ones(m + 2) / delta_x**2\n",
    "A += numpy.diag(diagonal * -2.0, 0)\n",
    "A += numpy.diag(diagonal[:-1], 1)\n",
    "A += numpy.diag(diagonal[:-1], -1)\n",
    "\n",
    "# Construct RHS\n",
    "b = f(x_bc)\n",
    "\n",
    "# Boundary conditions\n",
    "A[0, 0] = -1.0 / delta_x\n",
    "A[0, 1] = 1.0 / delta_x\n",
    "A[-1, -1] = -1.0 / (delta_x)\n",
    "A[-1, -2] =  1.0 / (delta_x)\n",
    "\n",
    "b[0] = alpha\n",
    "b[-1] = delta_x / 2.0 * f(x_bc[-1]) - sigma\n",
    "\n",
    "# Solve system\n",
    "U = numpy.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see why $A$ is singular, the constant vector $e = [1, 1, 1, 1, 1,\\ldots, 1]^T$ is in fact in the null-space of $A$.  Our numerical method has actually demonstrated this problem is *ill-posed*!  Indeed, since the boundary conditions are only on the derivatives there are an infinite number of solutions to the BVP (this could also occur if there were no solutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another way to understand why this is the case is to examine this problem again as the steady-state problem originating with the heat equation.  Consider the heat equation with $\\sigma_0 = \\sigma_1 = 0$ and $f(x) = 0$.  This setup would preserve and heat in the rod as none can escape through the ends of the rod.  In fact, the solution to the steady-state problem would simply redistribute the heat in the rod evenly across the rod based on the initial condition.  We then would have a solution\n",
    "$$\n",
    "    u(x) = \\int^1_0 u^0(x) dx = C.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem comes from the fact that the steady-state problem does not know about this bit of information by itself.  This means that the BVP as it stands could pick out any $C$, and it would be a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The solution is similar if we had the same setup except $f(x) \\neq 0$.  Now we are either adding or subtracting heat in the rod.  In this case there may not be a steady state at all!  You can actually show that if the addition and subtraction of heat exactly cancels we may in fact have a solution if\n",
    "$$\n",
    "    \\int^1_0 f(x) dx = 0\n",
    "$$\n",
    "which leads again to an infinite number of solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Linear Second Order Discretization\n",
    "\n",
    "Let's now describe a method for solving the equation\n",
    "$$\n",
    "    a(x) u''(x) + b(x) u'(x) + c(x) u(x) = f(x) ~~~~ \\Omega = [a, b] \\\\\n",
    "    u(a) = \\alpha ~~~~ u(b) = \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Try discretizing this using second order finite differences and write the system for\n",
    "$$\n",
    "    a(x) u''(x) + b(x) u'(x) + c(x) u(x) = f(x) ~~~~ \\Omega = [a, b] \\\\\n",
    "    u(a) = \\alpha ~~~~ u(b) = \\beta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-Linear Equations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
